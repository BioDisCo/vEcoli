{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pprint\n",
    "import ast\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import dill\n",
    "import requests\n",
    "import xmltodict\n",
    "import json\n",
    "\n",
    "pp = pprint.PrettyPrinter(depth=6)\n",
    "\n",
    "os.chdir(os.path.expanduser('~/vivarium-ecoli'))\n",
    "\n",
    "ALLOWED_METAL_NAMES =   {'Iron': 'FE+2', 'Cobalt': 'CO+2', 'Copper': 'CU+2', 'Manganese': 'MN+2', 'Molybdenum': 'CPD-8123', 'Nickel': 'NI+2', 'Zinc': 'ZN+2',\n",
    "                        'Calcium': 'CA+2', 'Magnesium': 'MG+2', 'Sodium': 'NA+', 'Potassium': 'K+',\n",
    "                        'Iron-sulfur \\(4Fe-4S\\)': 'CPD-7', 'Iron-sulfur \\(2Fe-2S\\)': 'CPD-6',\n",
    "                         'Iron-sulfur \\(4Fe-4S-S-AdoMet\\)': 'CPD-7', 'Iron-sulfur \\(3Fe-4S\\)': '3FE-4S', 'Iron-oxo-sulfur \\(4Fe-2O-2S\\)': 'CPD-7',\n",
    "                         'Iron-sulfur': 'CPD-7', # has to be after others since it is a substring of others\n",
    "                        'heme': 'Heme-b', 'Molybdate': 'CPD-3', 'heme B': 'Heme-b', 'Cobalamin': 'COB-I-ALAMIN',\n",
    "                         'Selenocysteine': 'L-SELENOCYSTEINE',\n",
    "                        'Divalent metal cation': 'Any+2'}\n",
    "\n",
    "\n",
    "ACCEPTED_OTHER_FEATURES = {'PYRIDOXAL_PHOSPHATE', 'THIAMINE-PYROPHOSPHATE', 'FMN', 'FAD', 'LIPOIC-ACID', 'BIOTIN'}\n",
    "\n",
    "AMINO_ACID_MAP = {'A': 'ALA', 'C': 'CYS', 'D': 'ASP', 'E': 'GLU', 'F': 'PHE', 'G': 'GLY', 'H': 'HIS', 'I': 'ILE',\n",
    "                  'K': 'LYS', 'L': 'LEU', 'M': 'MET', 'N': 'ASN', 'P': 'PRO', 'Q': 'GLN', 'R': 'ARG', 'S': 'SER',\n",
    "                  'T': 'THR', 'V': 'VAL', 'W': 'TRP', 'Y': 'TYR', 'U': 'SEL', '*': 'TER'}\n",
    "\n",
    "# Create a list of temporarily allowed Gene Ontology terms to fix gaps in pathway annotations. Usually non-metabolic\n",
    "# Currently: Tx Reg, Transcription, translation, DNA replication, Cell division, iron-sulfur cluster assembly, proteolysis, dna repair, copper response\n",
    "TEMP_GO_TERMS = {'GO:0006355': 'Regulation of transcription', 'GO:0010468': 'Regulation of transcription',\n",
    "                  'GO:0006351': 'Transcription', 'GO:0006350': 'Transcription',\n",
    "                  'GO:0006412': 'Translation', 'GO:0006260': 'DNA replication', 'GO:0006457': 'Protein folding', 'GO:0045454': 'Redox homeostasis',\n",
    "                  'GO:0051301': 'Cell division', 'GO:0015288': 'Porin',\n",
    "                  'GO:0016226': 'Iron-sulfur cluster assembly', 'GO:0006508': 'Proteolysis', 'GO:0006281': 'DNA repair',\n",
    "                  'GO:0046688': 'Response to copper ion'}\n",
    "\n",
    "\n",
    "# as residues\n",
    "AMINO_ACID_RESIDUE_MASSES = {\n",
    "    'ALA': 71.03711, 'ARG': 156.10111, 'ASN': 114.04293, 'ASP': 115.02694,\n",
    "    'CYS': 103.00919, 'GLU': 129.04259, 'GLN': 128.05858, 'GLY': 57.02146,\n",
    "    'HIS': 137.05891, 'ILE': 113.08406, 'LEU': 113.08406, 'LYS': 128.09496,\n",
    "    'MET': 131.04049, 'PHE': 147.06841, 'PRO': 97.05276, 'SER': 87.03203,\n",
    "    'THR': 101.04768, 'TRP': 186.07931, 'TYR': 163.06333, 'VAL': 99.06841,\n",
    "    'SEL': 150.0379, 'TER': 0.0\n",
    "}\n",
    "\n",
    "\n",
    "erroneous_monomer_metal_interactions = [\"EG11415-MONOMER\", \"EG12132-MONOMER\",\n",
    "    \"EG12332-MONOMER\", \"EG11378-MONOMER\", \"G7748-MONOMER\", \"CRR-MONOMER\", \"EG11663-MONOMER\", \"GLYOXI-MONOMER\"]\n",
    "\n",
    "def get_pathway_ith_level_parents(cur_pathway_idx, pathway_matrix, name_list, level_vector, level=2, parent_dict=None):\n",
    "\n",
    "    if parent_dict is None:\n",
    "        parent_dict = {}\n",
    "\n",
    "    cur_pathway_level = level_vector[cur_pathway_idx]\n",
    "\n",
    "    if cur_pathway_level == level:\n",
    "        parent_dict[name_list[cur_pathway_idx]] = cur_pathway_level\n",
    "\n",
    "    parent_slice = pathway_matrix[:, cur_pathway_idx]\n",
    "    parent_idxs = np.where(parent_slice != 0)[0]\n",
    "\n",
    "\n",
    "    for idx in parent_idxs:\n",
    "\n",
    "        _ = get_pathway_ith_level_parents(idx, pathway_matrix, name_list, level_vector, level, parent_dict)\n",
    "\n",
    "    return parent_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reload data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parsed_complex_df = pd.read_csv('notebooks/cofactors/data/raw_complexes.csv', index_col=False)\n",
    "\n",
    "# read stoichiometry, cofactors and enzyme_reaction as literal sets\n",
    "for column in ['stoichiometry', 'cofactors', 'enzyme_reaction']:\n",
    "    parsed_complex_df[column] = parsed_complex_df[column].apply(ast.literal_eval)\n",
    "\n",
    "parsed_protein_df = pd.read_csv('notebooks/cofactors/data/raw_proteins.csv', index_col=False)\n",
    "\n",
    "for column in ['cofactors', 'enzyme_reaction', 'metal_features', 'other_features', 'direct_annotations', 'go_annotations']:\n",
    "    parsed_protein_df[column] = parsed_protein_df[column].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "parsed_cofactor_df = pd.read_csv('notebooks/cofactors/data/raw_cofactors.csv', index_col=False)\n",
    "\n",
    "for column in ['elemental_composition']:\n",
    "    parsed_cofactor_df[column] = parsed_cofactor_df[column].apply(ast.literal_eval)\n",
    "\n",
    "parsed_pathway_df = pd.read_csv('notebooks/cofactors/data/raw_pathways.csv', index_col=False)\n",
    "\n",
    "for column in ['parents', 'children']:\n",
    "    parsed_pathway_df[column] = parsed_pathway_df[column].apply(ast.literal_eval)\n",
    "\n",
    "# add GO \"pathways\" to pathway df\n",
    "go_collection = []\n",
    "go_collection.append({'id': 'Other functions', 'common_name': 'Other functions', 'level': 1, 'parents': [], 'children': []})\n",
    "\n",
    "for go_term, go_name in TEMP_GO_TERMS.items():\n",
    "    go_collection[0]['children'].append(go_name)\n",
    "    go_collection.append({'id': go_name, 'common_name': go_name, 'level': 2, 'parents': ['Other functions'], 'children': []})\n",
    "\n",
    "go_df = pd.DataFrame(go_collection)\n",
    "\n",
    "parsed_pathway_df = pd.concat([parsed_pathway_df, go_df], ignore_index=True)\n",
    "parsed_pathway_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data processing into final tables\n",
    "## Specific adjustments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# removing specific troublesome interactions that have been discovered in the data\n",
    "# should not be in model\n",
    "\n",
    "\n",
    "# remove metal features from EG11415-MONOMER in parsed_protein_df\\\n",
    "for protein in erroneous_monomer_metal_interactions:\n",
    "    prot_idx = np.where(parsed_protein_df['id'] == protein)[0][0]\n",
    "    parsed_protein_df.at[prot_idx, 'metal_features'] = []\n",
    "\n",
    "# classify folE gene use as cofactor production (THF)\n",
    "pathway_idx = parsed_pathway_df[parsed_pathway_df['id'] == '6-HM-Dihydropterin-PP-Biosynthesis'].index[0]\n",
    "parsed_pathway_df.at[pathway_idx, 'parents'] = ['Cofactor-Biosynthesis']\n",
    "\n",
    "protein_idx = parsed_protein_df[parsed_protein_df['id'] == 'GTP-CYCLOHYDRO-I-MONOMER'].index[0]\n",
    "parsed_protein_df.at[protein_idx, 'direct_annotations'] = set(['PWY-6147'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "set(['PWY-6147'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process raw EcoCyc annotations into standard EcoCyc names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# remove all \\ characters from keys in ALLOWED_METAL_NAMES\n",
    "NON_REGEX_METAL = {key.replace('\\\\', ''): value for key, value in ALLOWED_METAL_NAMES.items()}\n",
    "\n",
    "parsed_protein_df['metal_features_processed'] = 0\n",
    "parsed_protein_df['metal_features_processed'] = parsed_protein_df['metal_features_processed'].astype(object)\n",
    "\n",
    "metal_pattern = '|'.join(ALLOWED_METAL_NAMES.keys())\n",
    "metal_regex = re.compile(f'(({metal_pattern})(\\s\\d[\\.,;]|[\\.,;]|\\s\\())')\n",
    "\n",
    "\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "\n",
    "    metal_binding = parsed_protein_df.loc[i, 'metal_features']\n",
    "\n",
    "    metal_count_dict = {}\n",
    "    existing_matches = set()\n",
    "\n",
    "    for feature in metal_binding:\n",
    "        matches = metal_regex.search(feature)\n",
    "        if matches:\n",
    "            metal = matches.group(0)[:-1]\n",
    "\n",
    "            # eliminate duplicates\n",
    "            if metal not in existing_matches:\n",
    "\n",
    "                existing_matches.add(metal)\n",
    "\n",
    "                if 'heme' in feature:\n",
    "                    metal = metal.replace('Iron', 'heme')\n",
    "                if 'alamin' in feature:\n",
    "                    metal = metal.replace('Cobalt', 'Cobalamin')\n",
    "\n",
    "                # check if last char of metal is a number, then crop\n",
    "                if metal[-1].isdigit():\n",
    "                    metal = metal[:-2]\n",
    "\n",
    "                metal = metal.strip()\n",
    "\n",
    "                # replace metal name with allowed metal name\n",
    "                metal = NON_REGEX_METAL[metal]\n",
    "\n",
    "                if metal in metal_count_dict:\n",
    "                    metal_count_dict[metal] += 1\n",
    "                else:\n",
    "                    metal_count_dict[metal] = 1\n",
    "\n",
    "        else:\n",
    "            print(f'No match for {feature} in {parsed_protein_df.loc[i, \"id\"]}')\n",
    "\n",
    "\n",
    "\n",
    "    # EXCEPTIONS\n",
    "    # if both magnesium and manganese are present, replace with magnesium\n",
    "    # TODO remove when using UniProt data. Ecocyc data is not as reliable\n",
    "    if 'MG+2' in metal_count_dict and 'MN+2' in metal_count_dict and metal_count_dict['MG+2'] == metal_count_dict['MN+2']:\n",
    "        del metal_count_dict['MN+2']\n",
    "    # same with cobalt\n",
    "    if 'CO+2' in metal_count_dict and 'MG+2' in metal_count_dict and metal_count_dict['CO+2'] == metal_count_dict['MG+2']:\n",
    "        del metal_count_dict['CO+2']\n",
    "    elif 'CO+2' in metal_count_dict:\n",
    "        metal_count_dict['MG+2'] = metal_count_dict['CO+2']\n",
    "        del metal_count_dict['CO+2']\n",
    "\n",
    "    parsed_protein_df.at[i, 'metal_features_processed'] = metal_count_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# manual corrections in data\n",
    "correction_idx = parsed_protein_df.index[parsed_protein_df['id'] == '3-OXOACYL-ACP-REDUCT-MONOMER'][0]\n",
    "\n",
    "# remove cofactors\n",
    "parsed_protein_df.at[correction_idx, 'metal_features_processed'] = {}\n",
    "\n",
    "correction_idx = parsed_protein_df.index[parsed_protein_df['id'] == 'CARBPSYN-LARGE'][0]\n",
    "\n",
    "# remove cofactors\n",
    "parsed_protein_df.at[correction_idx, 'metal_features_processed']\n",
    "mn_cofactor_count = parsed_protein_df.at[correction_idx, 'metal_features_processed']['MN+2']\n",
    "parsed_protein_df.at[correction_idx, 'metal_features_processed'] = {'MG+2': mn_cofactor_count}\n",
    "\n",
    "# dps\n",
    "correction_idx = parsed_protein_df.index[parsed_protein_df['id'] == 'EG11415-MONOMER'][0]\n",
    "parsed_protein_df.at[correction_idx, 'metal_features_processed'] = {}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parsed_protein_df = parsed_protein_df.drop(columns=['metal_features'])\n",
    "parsed_protein_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parsed_protein_df['other_features_processed'] = 0\n",
    "parsed_protein_df['other_features_processed'] = parsed_protein_df['other_features_processed'].astype(object)\n",
    "\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "\n",
    "    other_features = parsed_protein_df.loc[i, 'other_features']\n",
    "\n",
    "    other_feature_count_dict = {}\n",
    "    existing_matches = set()\n",
    "\n",
    "    for feature in other_features:\n",
    "\n",
    "        # eliminate duplicates\n",
    "        if feature not in existing_matches:\n",
    "\n",
    "            existing_matches.add(feature)\n",
    "\n",
    "            if feature in ACCEPTED_OTHER_FEATURES:\n",
    "                if feature in other_feature_count_dict:\n",
    "                    other_feature_count_dict[feature] += 1\n",
    "                else:\n",
    "                    other_feature_count_dict[feature] = 1\n",
    "\n",
    "    parsed_protein_df.at[i, 'other_features_processed'] = other_feature_count_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# processed go_annotations. if no pathways exist for monomer, use go annotations\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "\n",
    "    cur_go_annotations = parsed_protein_df.loc[i, 'go_annotations']\n",
    "    cur_pathways = parsed_protein_df.loc[i, 'direct_annotations']\n",
    "\n",
    "    replacement_pathways = list()\n",
    "\n",
    "    if len(cur_pathways) == 0:\n",
    "\n",
    "        for go_term in TEMP_GO_TERMS:\n",
    "            for go_annotation in cur_go_annotations:\n",
    "                if go_term == go_annotation:\n",
    "                    replacement_pathways.append(TEMP_GO_TERMS[go_term])\n",
    "\n",
    "        if len(replacement_pathways) > 0:\n",
    "            print(parsed_protein_df.at[i, \"id\"], set([replacement_pathways[0]]))\n",
    "            parsed_protein_df.at[i, 'direct_annotations'] = set([replacement_pathways[0]])\n",
    "\n",
    "\n",
    "# parsed_protein_df[parsed_protein_df['id'] == 'PD00197']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# decompose sequence into dict of amino acid counts\n",
    "parsed_protein_df['sequence_processed'] = 0\n",
    "parsed_protein_df['sequence_processed'] = parsed_protein_df['sequence_processed'].astype(object)\n",
    "\n",
    "parsed_protein_df['sequence_mass'] = 0\n",
    "\n",
    "unique_aa = set()\n",
    "\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "\n",
    "    sequence = parsed_protein_df.loc[i, 'seq']\n",
    "\n",
    "    if type(sequence) != str:\n",
    "        print(f'No sequence for {parsed_protein_df.loc[i, \"id\"]}')\n",
    "        continue\n",
    "\n",
    "    aa_count_dict = {}\n",
    "\n",
    "    mass = 0\n",
    "\n",
    "    for aa in sequence:\n",
    "        if aa in aa_count_dict:\n",
    "            aa_count_dict[aa] += 1\n",
    "        else:\n",
    "            aa_count_dict[aa] = 1\n",
    "\n",
    "        if aa not in unique_aa:\n",
    "            unique_aa.add(aa)\n",
    "\n",
    "        mass += AMINO_ACID_RESIDUE_MASSES[AMINO_ACID_MAP[aa]]\n",
    "\n",
    "    parsed_protein_df.at[i, 'sequence_processed'] = aa_count_dict\n",
    "    parsed_protein_df.at[i, 'sequence_mass'] = mass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parsed_protein_df = parsed_protein_df.drop(columns=['other_features','seq'])\n",
    "\n",
    "parsed_protein_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create new column for monomer component stoichiometry"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "complex_ids = parsed_complex_df['id'].tolist()\n",
    "monomer_names = parsed_protein_df['id'].tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recursive_component_tree(current_component_name, complex_table, protein_table,\n",
    "                             current_multiplier=1, component_list=None, parent=None, return_cofactors=False):\n",
    "    \"\"\"\n",
    "    Recursively find all downstream components of a given complex.\n",
    "    \"\"\"\n",
    "\n",
    "    complex_names = complex_table['id'].tolist()\n",
    "    monomer_names = protein_table['id'].tolist()\n",
    "\n",
    "\n",
    "    my_children = {}\n",
    "\n",
    "    if component_list is None:\n",
    "        component_list = []\n",
    "\n",
    "\n",
    "    if current_component_name in complex_names:\n",
    "\n",
    "\n",
    "        cplx_idx = complex_table.index[complex_table['id'] == current_component_name][0]\n",
    "        stoichiometry = complex_table.at[cplx_idx, 'stoichiometry']\n",
    "\n",
    "        direct_children = {k: abs(v) for k, v in stoichiometry.items() if v < 0}\n",
    "\n",
    "        for component_name, coefficient in stoichiometry.items():\n",
    "\n",
    "            if coefficient < 0 and component_name != current_component_name:\n",
    "\n",
    "                child_multiplier = abs(coefficient * current_multiplier)\n",
    "\n",
    "                new_child = recursive_component_tree(component_name, complex_table, protein_table,\n",
    "                                                     child_multiplier, component_list, current_component_name, return_cofactors)\n",
    "\n",
    "                my_children = my_children | new_child\n",
    "\n",
    "\n",
    "            elif coefficient > 0 and component_name == current_component_name:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"key {component_name} and value {coefficient} for complex {component_name} not processed properly.\")\n",
    "\n",
    "        component_list.append({'name': current_component_name,'parent': parent, 'children': direct_children,\n",
    "                               'multiplier': int(current_multiplier), })\n",
    "\n",
    "\n",
    "    elif current_component_name in monomer_names:\n",
    "\n",
    "        # TODO check if enzrxn\n",
    "        if return_cofactors:\n",
    "            protein_idx = protein_table.index[protein_table['id'] == current_component_name][0]\n",
    "\n",
    "            protein_metals = protein_table.at[protein_idx, 'metal_features_processed']\n",
    "            protein_other = protein_table.at[protein_idx, 'other_features_processed']\n",
    "\n",
    "            table_cofactors = protein_metals | protein_other\n",
    "\n",
    "            if len(table_cofactors) > 0:\n",
    "                # TODO Add apo protein to component list\n",
    "                my_children = {}\n",
    "\n",
    "                for cofactor, cofactor_coefficient in table_cofactors.items():\n",
    "                    if table_cofactors[cofactor] !=  None:\n",
    "                        my_children[cofactor] = cofactor_coefficient\n",
    "                        component_list.append({'parent': current_component_name,\n",
    "                                               'name': cofactor,\n",
    "                                               'multiplier': abs(int(current_multiplier * cofactor_coefficient)),\n",
    "                                               'children': None})\n",
    "\n",
    "            component_list.append({'parent': parent, 'name': current_component_name, 'multiplier': current_multiplier, 'children': my_children})\n",
    "\n",
    "        else:\n",
    "            my_children = None\n",
    "            component_list.append({'parent': parent, 'name': current_component_name, 'multiplier': current_multiplier, 'children': None})\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(f\"component {current_component_name} not found in complex or protein tables\")\n",
    "\n",
    "        return {}\n",
    "\n",
    "\n",
    "    if parent is None:\n",
    "        return {current_component_name: my_children}, component_list\n",
    "    else:\n",
    "        return {current_component_name: my_children}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "complex_tree_structure, nodes = recursive_component_tree('CPLX0-8167', parsed_complex_df, parsed_protein_df)\n",
    "pp.pprint(nodes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parsed_complex_df['monomer_component_stoichiometry'] = 0\n",
    "parsed_complex_df['monomer_component_stoichiometry'] = parsed_complex_df['monomer_component_stoichiometry'].astype(object)\n",
    "\n",
    "for i in range(len(parsed_complex_df.index)):\n",
    "    complex_name = parsed_complex_df.loc[i, 'id']\n",
    "    complex_tree_structure, nodes = recursive_component_tree(complex_name, parsed_complex_df, parsed_protein_df)\n",
    "\n",
    "    monomer_components = {node['name']: node['multiplier'] for node in nodes if node['children'] is None}\n",
    "\n",
    "    parsed_complex_df.at[i, 'monomer_component_stoichiometry'] = monomer_components"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parsed_complex_df = parsed_complex_df.loc[:, [\"id\", \"common_name\", \"stoichiometry\", \"monomer_component_stoichiometry\", \"cofactors\"]]\n",
    "parsed_complex_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create tree matrix (also for Julia)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save names\n",
    "complex_ids = list(parsed_complex_df['id'])\n",
    "protein_ids = list(parsed_protein_df['id'])\n",
    "cofactor_ids = list(parsed_cofactor_df['id'])\n",
    "pathway_ids = list(parsed_pathway_df['id'])\n",
    "\n",
    "\n",
    "name_idx = complex_ids + protein_ids + cofactor_ids\n",
    "tree_matrix = np.zeros([len(complex_ids) + len(protein_ids) + len(cofactor_ids), len(complex_ids) + len(protein_ids) + len(cofactor_ids)], dtype=np.int64)\n",
    "\n",
    "for i in range(len(parsed_complex_df)):\n",
    "    name = parsed_complex_df.at[i, 'id']\n",
    "    tree_structure, nodes = recursive_component_tree(name, parsed_complex_df, parsed_protein_df, return_cofactors=True)\n",
    "\n",
    "    for node in nodes:\n",
    "        node_name = node['name']\n",
    "        node_children = node['children']\n",
    "\n",
    "        if node_children != None:\n",
    "            for child_name, child_coefficient in node_children.items():\n",
    "                if child_name in name_idx:\n",
    "                        tree_matrix[name_idx.index(node_name), name_idx.index(child_name)] = child_coefficient"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create matrices to get cofactor counts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parsed_pathway_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "priority_list = ['Activation-Inactivation-Interconversion','Metabolic-Clusters', 'Macromolecule-Modification', 'Glycan-Pathways','Detoxification',  'Degradation']\n",
    "\n",
    "# pathway matrix is necessary to traverse tree\n",
    "pathway_matrix = np.zeros((len(pathway_ids), len(pathway_ids)), dtype=np.int64)\n",
    "level_vector = np.zeros(len(pathway_ids), dtype=np.int64)\n",
    "\n",
    "for i in range(len(parsed_pathway_df)):\n",
    "\n",
    "    cur_pathway = parsed_pathway_df.at[i, 'id']\n",
    "    level_vector[i] = parsed_pathway_df.at[i, 'level']\n",
    "\n",
    "    pathway_parents = parsed_pathway_df.at[i, 'parents']\n",
    "    pathway_children = parsed_pathway_df.at[i, 'children']\n",
    "\n",
    "    for parent in pathway_parents:\n",
    "        j = pathway_ids.index(parent)\n",
    "        pathway_matrix[j, i] = 1\n",
    "\n",
    "    for child in pathway_children:\n",
    "        j = pathway_ids.index(child)\n",
    "        pathway_matrix[i, j] = 1\n",
    "\n",
    "original_pathway_matrix = pathway_matrix.copy()\n",
    "\n",
    "# get superpathway indices\n",
    "super_pathway_idx = pathway_ids.index('Super-Pathways')\n",
    "super_pathway_children_idxs = np.where(pathway_matrix[super_pathway_idx, :] == 1)[0]\n",
    "\n",
    "# zero out all superpathway children\n",
    "pathway_matrix[:, super_pathway_children_idxs] = 0\n",
    "\n",
    "# for columns (children) with multiple parents, if one parent leads to degradation or glycans, remove it.\n",
    "for i in range(len(pathway_matrix[0, :])):\n",
    "    cur_pathway = pathway_ids[i]\n",
    "\n",
    "    if pathway_matrix[:, i].sum() > 1:\n",
    "\n",
    "        nz_idxs = np.where(pathway_matrix[:, i] == 1)[0]\n",
    "        top_level_classes = [list(get_pathway_ith_level_parents(j, original_pathway_matrix, pathway_ids, level_vector, level=1).keys())[0] for j in nz_idxs]\n",
    "        # print(f\"multiple parents {top_level_classes} for {cur_pathway}\")\n",
    "\n",
    "\n",
    "        # when there are multiple parents, remove them in the following order of priority:\n",
    "        for priority in priority_list:\n",
    "            while priority in top_level_classes and len(nz_idxs) > 1:\n",
    "                priority_index = top_level_classes.index(priority)\n",
    "                pathway_matrix[nz_idxs[priority_index], i] = 0\n",
    "                nz_idxs = np.where(pathway_matrix[:, i] == 1)[0]\n",
    "                top_level_classes[priority_index] = 'N/A'\n",
    "\n",
    "\n",
    "        # then, if there are still multiple parents, remove all but the first one\n",
    "        # TODO Change to parent with most frequently occuring 2nd parent.\n",
    "        if len(nz_idxs) > 1:\n",
    "            # top_two_level_classes = [list(get_pathway_ith_level_parents(j, original_pathway_matrix, pathway_name_list, level_vector, level=2).keys())[0] for j in nz_idxs]\n",
    "            # print(f\"multiple parents with 2nd level categories {top_two_level_classes} for {cur_pathway}\")\n",
    "            pathway_matrix[nz_idxs[1:], i] = 0\n",
    "\n",
    "        nz_idxs = np.where(pathway_matrix[:, i] == 1)[0]\n",
    "        top_level_classes = [list(get_pathway_ith_level_parents(j, original_pathway_matrix, pathway_ids, level_vector, level=1).keys()) for j in nz_idxs]\n",
    "\n",
    "        # print(f\"pruned to {top_level_classes}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# if choice can be made, don't pick these.\n",
    "priority_list_second = ['SECONDARY-METABOLITE-BIOSYNTHESIS', 'Respiration']\n",
    "\n",
    "# create protein name to pathway mapping\n",
    "W = np.zeros((len(parsed_protein_df.index), len(parsed_pathway_df.index)))\n",
    "\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "\n",
    "    cur_pathways = parsed_protein_df.at[i, 'direct_annotations']\n",
    "\n",
    "    for pathway in cur_pathways:\n",
    "        pathway_idx = pathway_ids.index(pathway)\n",
    "        W[i, pathway_idx] = 1\n",
    "\n",
    "\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "\n",
    "    cur_protein_pathways_idxs = np.where(W[i, :] == 1)[0]\n",
    "    cur_protein = parsed_protein_df.at[i, 'id']\n",
    "\n",
    "    if len(cur_protein_pathways_idxs) < 2:\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "\n",
    "        # for pathway_idx in cur_protein_pathways_idxs:\n",
    "\n",
    "        # get top level class of every pathway, and remove all but the first one of each class.\n",
    "\n",
    "        cur_protein_pathway_parents = list()\n",
    "        cur_protein_pathway_two_parents = list()\n",
    "\n",
    "        for pathway_idx in cur_protein_pathways_idxs:\n",
    "            top_parents = get_pathway_ith_level_parents(pathway_idx, pathway_matrix, pathway_ids, level_vector, level=1)\n",
    "            top_two_parents = get_pathway_ith_level_parents(pathway_idx, pathway_matrix, pathway_ids, level_vector, level=2)\n",
    "\n",
    "            print(f\"{cur_protein} has 2nd parents {top_two_parents} for {pathway_ids[pathway_idx]}\")\n",
    "\n",
    "            if len(top_parents) > 1:\n",
    "                print(f\"multiple parents {top_parents} for {pathway_ids[pathway_idx]} for {cur_protein}, should not happen.\")\n",
    "\n",
    "            if len(top_parents) == 1:\n",
    "                cur_protein_pathway_parents.append(list(top_parents.keys())[0])\n",
    "                cur_protein_pathway_two_parents.append(list(top_two_parents.keys())[0])\n",
    "            else:\n",
    "                # remove pathway with no parents\n",
    "                W[i, pathway_idx] = 0\n",
    "                cur_protein_pathway_parents.append('N/A')\n",
    "                cur_protein_pathway_two_parents.append('N/A')\n",
    "\n",
    "\n",
    "        # TODO - remove direct annotations with deprioritized parents\n",
    "        for priority in priority_list:\n",
    "            while priority in cur_protein_pathway_parents and len(np.where(W[i, :] == 1)[0]) > 1:\n",
    "                priority_index = cur_protein_pathway_parents.index(priority)\n",
    "                W[i, cur_protein_pathways_idxs[priority_index]] = 0\n",
    "                cur_protein_pathway_parents[priority_index] = 'N/A'\n",
    "\n",
    "        # same for 2nd level\n",
    "        for priority in priority_list_second:\n",
    "            while priority in cur_protein_pathway_two_parents and len(np.where(W[i, :] == 1)[0]) > 1:\n",
    "                priority_index = cur_protein_pathway_two_parents.index(priority)\n",
    "                W[i, cur_protein_pathways_idxs[priority_index]] = 0\n",
    "                cur_protein_pathway_two_parents[priority_index] = 'N/A'\n",
    "\n",
    "        new_protein_pathways_idxs = np.where(W[i, :] == 1)[0]\n",
    "\n",
    "        # for pathway_idx in cur_protein_pathways_idxs:\n",
    "\n",
    "        # get top level class of every pathway, and remove all but the first one of each class.\n",
    "\n",
    "        new_protein_pathway_parents = list()\n",
    "        new_protein_pathway_two_parents = list()\n",
    "\n",
    "        for pathway_idx in new_protein_pathways_idxs:\n",
    "            top_parents = get_pathway_ith_level_parents(pathway_idx, pathway_matrix, pathway_ids, level_vector, level=1)\n",
    "            top_two_parents = get_pathway_ith_level_parents(pathway_idx, pathway_matrix, pathway_ids, level_vector, level=2)\n",
    "\n",
    "            if len(top_parents) == 1:\n",
    "                new_protein_pathway_parents.append(list(top_parents.keys())[0])\n",
    "                new_protein_pathway_two_parents.append(list(top_two_parents.keys())[0])\n",
    "\n",
    "        print(f\"pruned to {cur_protein_pathway_two_parents}\")\n",
    "\n",
    "\n",
    "        # remove all N/A\n",
    "        # cur_protein_pathways_idxs = cur_protein_pathways_idxs[cur_protein_pathway_parents != 'N/A']\n",
    "\n",
    "        if len(np.unique(cur_protein_pathway_parents)) < 2:\n",
    "            continue\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create pathway to 2nd layer pathway mapping\n",
    "W2 = np.zeros((len(parsed_pathway_df.index), len(parsed_pathway_df.index)))\n",
    "\n",
    "for i in range(len(parsed_pathway_df.index)):\n",
    "\n",
    "    cur_pathway = parsed_pathway_df.at[i, 'id']\n",
    "    cur_pathway_idx = pathway_ids.index(cur_pathway)\n",
    "\n",
    "    # get 2nd level parents\n",
    "    cur_pathway_parents = get_pathway_ith_level_parents(cur_pathway_idx, pathway_matrix, pathway_ids, level_vector, level=2)\n",
    "\n",
    "    # if len(cur_pathway_parents) > 1:\n",
    "    #     print(f\"cur pathway {cur_pathway} has parents {cur_pathway_parents}\")\n",
    "\n",
    "    for parent in cur_pathway_parents:\n",
    "        parent_idx = pathway_ids.index(parent)\n",
    "        W2[i, parent_idx] = 1\n",
    "\n",
    "# zero diagonal (don't return self, since some pathways return themselves as level 2 parents)\n",
    "np.fill_diagonal(W2[0:(W2.shape[0] - len(go_collection)), 0:(W2.shape[0] - len(go_collection))], 0)\n",
    "\n",
    "\n",
    "W1 = np.zeros((len(parsed_pathway_df.index), len(parsed_pathway_df.index)))\n",
    "\n",
    "for i in range(len(parsed_pathway_df.index)):\n",
    "\n",
    "    if parsed_pathway_df.at[i, 'level'] <= 2:\n",
    "        cur_pathway = parsed_pathway_df.at[i, 'id']\n",
    "        cur_pathway_idx = pathway_ids.index(cur_pathway)\n",
    "\n",
    "        # get 2nd level parents\n",
    "        cur_pathway_parents = get_pathway_ith_level_parents(cur_pathway_idx, pathway_matrix, pathway_ids, level_vector, level=1)\n",
    "\n",
    "        for parent in cur_pathway_parents:\n",
    "            parent_idx = pathway_ids.index(parent)\n",
    "            W1[i, parent_idx] = 1\n",
    "\n",
    "np.fill_diagonal(W1, 0)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# C matrix transforms complexes + monomers to just monomers.\n",
    "\n",
    "# create protein name to index mapping\n",
    "protein_name_to_index = {}\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "    protein_name_to_index[parsed_protein_df.at[i, 'id']] = i\n",
    "\n",
    "# C matrix: complexes x proteins\n",
    "C = np.zeros((len(parsed_complex_df.index), len(parsed_protein_df.index)))\n",
    "\n",
    "for i in range(len(parsed_complex_df.index)):\n",
    "\n",
    "    complex_components = parsed_complex_df.loc[i, 'monomer_component_stoichiometry']\n",
    "\n",
    "    # TODO consider cofactors\n",
    "    # complex_cofactors = filter_complex_df.loc[i, 'cofactors']\n",
    "\n",
    "\n",
    "    for component_name, component_count in complex_components.items():\n",
    "        if component_count is not None:             # side effect of parquet\n",
    "            # get index of component in filter_protein_df\n",
    "            component_index = protein_name_to_index[component_name]\n",
    "\n",
    "            if parsed_complex_df.at[i, 'id'] == 'APORNAP-CPLX':\n",
    "                print(f'component_name: {component_name}, component_count: {component_count}, component_index: {component_index}')\n",
    "\n",
    "            C[i, component_index] = component_count\n",
    "\n",
    "# append an identity matrix to C\n",
    "C = np.concatenate((C, np.identity(len(parsed_protein_df.index))), axis=0)\n",
    "\n",
    "C_names = list(parsed_complex_df['id']) + list(parsed_protein_df['id'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# P matrix transforms proteins to their respective cofactor counts.\n",
    "\n",
    "# create cofactor name to index mapping\n",
    "cofactor_name_to_index = {}\n",
    "for i in range(len(parsed_cofactor_df.index)):\n",
    "    cofactor_name_to_index[parsed_cofactor_df.at[i, 'id']] = i\n",
    "\n",
    "cofactor_ids = list(parsed_cofactor_df['id'])\n",
    "\n",
    "# P matrix: proteins x cofactors\n",
    "P = np.zeros((len(parsed_protein_df.index), len(parsed_cofactor_df.index)))\n",
    "\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "    protein_metals = parsed_protein_df.loc[i, 'metal_features_processed']\n",
    "    protein_other = parsed_protein_df.loc[i, 'other_features_processed']\n",
    "\n",
    "    for metal, count in protein_metals.items():\n",
    "        if count is not None:             # side effect of parquet\n",
    "            cofactor_index = cofactor_name_to_index[metal]\n",
    "            P[i, cofactor_index] = count\n",
    "\n",
    "    for other, count in protein_other.items():\n",
    "        if count is not None:             # side effect of parquet\n",
    "            cofactor_index = cofactor_name_to_index[other]\n",
    "            P[i, cofactor_index] = count\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# E matrix transforms cofactors to their respective elemental composition\n",
    "\n",
    "# create list of unique elements\n",
    "unique_elements = set()\n",
    "for i in range(len(parsed_cofactor_df.index)):\n",
    "    cofactor = parsed_cofactor_df.at[i, 'elemental_composition']\n",
    "    unique_elements.update(cofactor.keys())\n",
    "\n",
    "unique_elements = list(unique_elements)\n",
    "\n",
    "# create E matrix: cofactors x elements\n",
    "E = np.zeros((len(parsed_cofactor_df.index), len(unique_elements)))\n",
    "\n",
    "for i in range(len(parsed_cofactor_df.index)):\n",
    "    cofactor = parsed_cofactor_df.at[i, 'elemental_composition']\n",
    "\n",
    "    for element, count in cofactor.items():\n",
    "        if count is not None:             # side effect of parquet\n",
    "            element_index = unique_elements.index(element)\n",
    "            E[i, element_index] = count\n",
    "\n",
    "\n",
    "element_ids = unique_elements"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A matrix transforms proteins to their amino acid composition\n",
    "\n",
    "A = np.zeros((len(parsed_protein_df.index), len(unique_aa)))\n",
    "\n",
    "amino_acid_single_letter = list(unique_aa)\n",
    "amino_acid_ids = [AMINO_ACID_MAP[aa] for aa in amino_acid_single_letter]\n",
    "\n",
    "for i in range(len(parsed_protein_df.index)):\n",
    "    protein = parsed_protein_df.at[i, 'sequence_processed']\n",
    "\n",
    "    if type(protein) is not dict:\n",
    "        continue\n",
    "\n",
    "    for aa, count in protein.items():\n",
    "        if count is not None:             # side effect of parquet\n",
    "            aa_index = amino_acid_single_letter.index(aa)\n",
    "            A[i, aa_index] = count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Now ... add the counts >:o"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time = '3000'\n",
    "date = '2023-10-31'\n",
    "experiment = 'metabolism-redux-classic'\n",
    "entry = f'{experiment}_{time}_{date}'\n",
    "folder = f'out/cofactors/{entry}/'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_all = np.load(folder + 'output.npy',allow_pickle='TRUE').item()\n",
    "# output = np.load(r\"out/geneRxnVerifData/output_glc.npy\", allow_pickle=True, encoding='ASCII').tolist()\n",
    "output = output_all['agents']['0']\n",
    "fba = output['listeners']['fba_results']\n",
    "mass = output['listeners']['mass']\n",
    "bulk = pd.DataFrame(output['bulk'])\n",
    "\n",
    "fluxes = np.array(fba['estimated_fluxes'][1:])\n",
    "exchanges = fba['estimated_exchange_dmdt']\n",
    "\n",
    "# output['listeners']['unique_molecule_counts']['active_ribosome']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output['listeners']['unique_molecule_counts']['active_RNAP']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f = open(folder + 'agent_steps.pkl', 'rb')\n",
    "agent = dill.load(f)\n",
    "f.close()\n",
    "\n",
    "metabolism = agent['ecoli-metabolism-redux-classic']\n",
    "stoichiometry = metabolism.stoichiometry\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "initial_state = json.load(open('data/wcecoli_t0.json'))\n",
    "\n",
    "bulk_ids = [item[0] for item in initial_state['bulk']]\n",
    "\n",
    "bulk.columns = bulk_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# in the bulk dataframe, update RNAP and 50S, 30S rib counts from unique molecules since they are sequestered as unique when in use.\n",
    "for unique_key, bulk_id in [('active_ribosome', 'CPLX0-3962'), ('active_ribosome', 'CPLX0-3953'), ('active_RNAP', 'APORNAP-CPLX')]:\n",
    "    if unique_key in output['listeners']['unique_molecule_counts']:\n",
    "        unique_count = output['listeners']['unique_molecule_counts'][unique_key]\n",
    "        bulk.loc[:, bulk_id+'[c]'] += unique_count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ecocyc_to_wcm_map = {}\n",
    "\n",
    "# combined complex and protein names\n",
    "complex_protein_names = list(parsed_protein_df['id']) + list(parsed_complex_df['id'])\n",
    "\n",
    "for name in complex_protein_names:\n",
    "\n",
    "    # find complex name in bulk_ids\n",
    "    found = False\n",
    "\n",
    "    try:\n",
    "        idx = bulk_ids.index(name+'[c]')\n",
    "        ecocyc_to_wcm_map[name] = name+'[c]'\n",
    "        found = True\n",
    "        # print(f'found {complex_name} at {idx}')\n",
    "\n",
    "    except ValueError:\n",
    "        # delete key\n",
    "        found = False\n",
    "\n",
    "\n",
    "    if found == False:\n",
    "\n",
    "        for id in bulk_ids:\n",
    "            if name+'[' in id and id.startswith(name) and bulk.loc[:, id].sum() > 0:\n",
    "                #print(f'found {name} in {id} with nonzero count')\n",
    "                ecocyc_to_wcm_map[name] = id\n",
    "                found = True\n",
    "                break           # ensures preferring nonzero counts\n",
    "\n",
    "            elif name+'[' in id and id.startswith(name):\n",
    "                # print(f'found {name} in {id} with zero count')\n",
    "                ecocyc_to_wcm_map[name] = id\n",
    "                found = True\n",
    "\n",
    "    if found == False:\n",
    "        ecocyc_to_wcm_map[name] = '--TRANS-ACENAPHTHENE-12-DIOL[c]' # should be none\n",
    "        print(f'could not find {name}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "complex_wcm_names = [ecocyc_to_wcm_map[name] for name in C_names]\n",
    "\n",
    "counts = bulk.loc[:, complex_wcm_names]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time = '1300'\n",
    "date = '2023-12-14'\n",
    "experiment = 'metabolism-redux-classic'\n",
    "entry = f'{experiment}_{time}_{date}'\n",
    "folder = f'out/cofactors/{entry}/'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_all = np.load(folder + '0_output.npy',allow_pickle='TRUE').item()\n",
    "# output = np.load(r\"out/geneRxnVerifData/output_glc.npy\", allow_pickle=True, encoding='ASCII').tolist()\n",
    "output = output_all['agents']['0']\n",
    "fba = output['listeners']['fba_results']\n",
    "mass = output['listeners']['mass']\n",
    "bulk = pd.DataFrame(output['bulk'])\n",
    "\n",
    "fluxes = np.array(fba['estimated_fluxes'][1:])\n",
    "exchanges = fba['estimated_exchange_dmdt']\n",
    "\n",
    "bulk_ids = [item[0] for item in initial_state['bulk']]\n",
    "\n",
    "bulk.columns = bulk_ids\n",
    "# output['listeners']['unique_molecule_counts']['active_ribosome']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# in the bulk dataframe, update RNAP and 50S, 30S rib counts from unique molecules since they are sequestered as unique when in use.\n",
    "for unique_key, bulk_id in [('active_ribosome', 'CPLX0-3962'), ('active_ribosome', 'CPLX0-3953'), ('active_RNAP', 'APORNAP-CPLX')]:\n",
    "    if unique_key in output['listeners']['unique_molecule_counts']:\n",
    "        unique_count = output['listeners']['unique_molecule_counts'][unique_key]\n",
    "        bulk.loc[:, bulk_id+'[c]'] += unique_count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rich_counts = bulk.loc[:, complex_wcm_names]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# External data sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get surface area occupancy estimates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# roughly 90 by 35 rectangle. pretty big? Multiplicity 2.\n",
    "# 0.009 x 0.0035 um^2 = 0.000032 um^2 * 3500 / 2 = 0.055 um^2\n",
    "# in rich media\n",
    "# 0.009 x 0.0035 um^2 = 0.000032 um^2 * 15000 / 2 = 0.24 um^2\n",
    "counts.at[0, ecocyc_to_wcm_map['CPLX-157']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 160 x 50 angstrom, = 0.016 x 0.005 um^2 = 0.00008 um^2 * 1000 = 0.08 um^2. Multiplicity 1\n",
    "# in rich media\n",
    "# 0.00008 um^2 * 3000 = 0.24 um^2, if 15000 would be like 20-30% of membrane space.\n",
    "counts.at[0, ecocyc_to_wcm_map['NADH-DHI-CPLX']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# roughly 70 by 40 rectangle. pretty big?, 0.007 x 0.004 um^2 = 0.000028 um^2 * 3000 ~= 0.09 um^2\n",
    "# grows to ~ 0.5 um^2 for 15000 counts in rich. Multiplicity 1. 8-15% of membrane space.\n",
    "counts.at[0, ecocyc_to_wcm_map['CYT-O-UBIOX-CPLX']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# atpsyn, 25 rad circle, plus roughly 25 width accessory circling around. Multiplicity 1\n",
    "# 0.005 um^2 * 3 = 0.000075 um^2 * 3500 = 0.26 um^2\n",
    "# in rich media\n",
    "# 0.005 um^2 * 3 = 0.000075 um^2 * 10000 = 0.75 um^2, 10-20% of membrane space.\n",
    "counts.at[0, ecocyc_to_wcm_map['ATPSYN-CPLX']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sdh, rough 37 angstrom radius, half filled circle. multiplicity 3\n",
    "# 0.0037 um^2 * 3 / (2*3) = 0.000006845 um^2 * 3500 = 0.024 um^2\n",
    "# in rich, about the same."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ftsh, roughly 100 angstrom radius, circle. Multiplicity 24. Small impact.\n",
    "# 0.01 um^2 * 3 = 0.0003 um^2 * 1000/24 = 0.0125 um^2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# secyeg, roughly 40x40 rectangle. Multiplicity 1.\n",
    "# 0.004 um^2  = 0.000016 um^2 * 1700 = 0.0272 um^2\n",
    "# in rich, increases sixfold to 0.16 um^2. Becomes around 2-5%. Not bad.\n",
    "\n",
    "# add secdf\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tcs, about 30-50 copies for 30 different systems. 8 angstrom radius for dimer. Multiplicity 2.\n",
    "# 0.0008 um^2 * 3= 0.00000192 um^2 * 30 * 30 / 2  = 0.000900 um^2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mscS, 40 angstrom radius, circle. 3000 copies. Multiplicity 7.\n",
    "# 0.004 um^2 * 3 = 0.000048 um^2 * 3000 / 7 = 0.0210 um^2\n",
    "# 1-2% of membrane. Not bad.\n",
    "\n",
    "# mscL, 20 angstrom radius, circle. 3000 copies. Multiplicity 5.\n",
    "# 0.002 um^2 * 3 = 0.000012 um^2 * 3000 / 5 = 0.0072 um^2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pntAB, 60x20 rectangle. 3000-6000 copies, multiplicity 2.\n",
    "# 0.006 * 0.002 = 0.000012 um^2 * 3000 / 2 = 0.018 um^2\n",
    "# 1-2% of membrane. Not bad."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read in surface area occupancy estimates\n",
    "area_df = pd.read_csv('notebooks/cofactors/external_data/membrane_areas.txt', skipinitialspace=True)\n",
    "area_df\n",
    "\n",
    "# strip whitespace from"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normal_surface_area = 4\n",
    "rich_surface_area = 8\n",
    "\n",
    "area_df['fraction_normal'] = (area_df['count_normal'] / area_df['multiplicity'] * area_df['estimated_area um2']) / normal_surface_area\n",
    "area_df['fraction_rich'] = (area_df['count_rich'] / area_df['multiplicity'] * area_df['estimated_area um2']) / rich_surface_area\n",
    "area_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "area_df['fraction_normal'].sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "area_df['fraction_rich'].sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "area_df['fraction_rich']  / area_df['fraction_rich'].sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Saving outputs to files compatible with Julia"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parsed_pathway_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "complex_names = list(parsed_complex_df['common_name'])\n",
    "protein_names = list(parsed_protein_df['common_name'])\n",
    "cofactor_names = list(parsed_cofactor_df['common_name'])\n",
    "pathway_names = list(parsed_pathway_df['common_name'])\n",
    "\n",
    "# if cplx name is nan then use id\n",
    "complex_names = [complex_names[i] if type(complex_names[i]) == str else parsed_complex_df['id'][i] for i in range(len(complex_names))]\n",
    "protein_names = [protein_names[i] if type(protein_names[i]) == str else parsed_protein_df['id'][i] for i in range(len(protein_names))]\n",
    "cofactor_names = [cofactor_names[i] if type(cofactor_names[i]) == str else parsed_cofactor_df['id'][i] for i in range(len(cofactor_names))]\n",
    "pathway_names = [pathway_names[i] if type(pathway_names[i]) == str else parsed_pathway_df['id'][i] for i in range(len(pathway_names))]\n",
    "\n",
    "# save C, P and E to julia-compatible formats\n",
    "np.savetxt('notebooks/cofactors/data/C_matrix.csv', C.astype(np.int64), delimiter=',', fmt='%i')\n",
    "np.savetxt('notebooks/cofactors/data/P_matrix.csv', P.astype(np.int64), delimiter=',', fmt='%i')\n",
    "np.savetxt('notebooks/cofactors/data/E_matrix.csv', E.astype(np.int64), delimiter=',', fmt='%i')\n",
    "np.savetxt('notebooks/cofactors/data/W_matrix.csv', W.astype(np.float64), delimiter=',')\n",
    "np.savetxt('notebooks/cofactors/data/W1_matrix.csv', W1.astype(np.float64), delimiter=',')\n",
    "np.savetxt('notebooks/cofactors/data/W2_matrix.csv', W2.astype(np.float64), delimiter=',')\n",
    "np.savetxt('notebooks/cofactors/data/A_matrix.csv', A.astype(np.float64), delimiter=',')\n",
    "\n",
    "\n",
    "# write all ids to single file with each list on a new line\n",
    "with open('notebooks/cofactors/data/complex_ids.txt', 'w') as f:\n",
    "    f.write('\\n'.join(complex_ids))\n",
    "with open('notebooks/cofactors/data/protein_ids.txt', 'w') as f:\n",
    "    f.write('\\n'.join(protein_ids))\n",
    "with open('notebooks/cofactors/data/cofactor_ids.txt', 'w') as f:\n",
    "    f.write('\\n'.join(cofactor_ids))\n",
    "with open('notebooks/cofactors/data/element_ids.txt', 'w') as f:\n",
    "    f.write('\\n'.join(element_ids))\n",
    "with open('notebooks/cofactors/data/pathway_ids.txt', 'w') as f:\n",
    "    f.write('\\n'.join(pathway_ids))\n",
    "with open('notebooks/cofactors/data/amino_acid_ids.txt', 'w') as f:\n",
    "    f.write('\\n'.join(amino_acid_ids))\n",
    "\n",
    "# write all names to single file with each list on a new line\n",
    "with open('notebooks/cofactors/data/complex_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(complex_names))\n",
    "with open('notebooks/cofactors/data/protein_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(protein_names))\n",
    "with open('notebooks/cofactors/data/cofactor_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(cofactor_names))\n",
    "with open('notebooks/cofactors/data/pathway_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(pathway_names))\n",
    "\n",
    "\n",
    "# save counts of proteins and complexes\n",
    "np.savetxt('notebooks/cofactors/data/counts.csv', np.array(counts, dtype=np.int64), delimiter=',', fmt='%i')\n",
    "\n",
    "# save counts of proteins and complexes\n",
    "np.savetxt('notebooks/cofactors/data/rich_counts.csv', np.array(rich_counts, dtype=np.int64), delimiter=',', fmt='%i')\n",
    "\n",
    "\n",
    "# save masses\n",
    "np.savetxt('notebooks/cofactors/data/monomer_masses.csv', np.array(parsed_protein_df['sequence_mass']), delimiter=',', fmt='%f')\n",
    "\n",
    "# save tree_matrix\n",
    "np.savetxt('notebooks/cofactors/data/tree_matrix.csv', tree_matrix, delimiter=',', fmt='%i')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save counts as csv with header for Khoa\n",
    "counts.to_csv('notebooks/cofactors/data/counts_header.csv', index=False)\n",
    "\n",
    "parsed_protein_df.to_csv('notebooks/cofactors/data/protein_table.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parsed_protein_df[parsed_protein_df['id'] == 'EG12332-MONOMER']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let's test that the cofactor counts are correct for SULFITE-REDUCT-CPLX and its constituent BETACOMP-MONOMER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(len(parsed_protein_df)):\n",
    "    sequence = parsed_protein_df.at[i, 'sequence_processed']\n",
    "\n",
    "    if type(sequence) is dict and 'U' in sequence:\n",
    "        print(f'found U in {parsed_protein_df.at[i, \"id\"]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Sulfite reductase complex count: {counts[1020]}, hemoprotein subunit count: {counts[1271]}')\n",
    "print(f'Expected iron count: {counts[1020] * 5 * 4} for the first, {counts[1271] * 5 } for the second')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check the pathway counts for the related pathway."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cplx_pathways = pathway_ids[ np.where((C @ W)[1020, :])[0][0] ]\n",
    "monomer_pathways = pathway_ids[ np.where((C @ W)[1271, :])[0][0] ]\n",
    "\n",
    "\n",
    "print(f'Sulfite reductase is in the following pathways: {cplx_pathways}, while the hemoprotein is in {monomer_pathways}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That means the pathway counts for iron should be the same as the cplx counts (or larger)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pathway_counts = (iron_counts @ C_n @ W)[  np.where((C @ W)[1020, :])[0][0]  ]\n",
    "\n",
    "# print(f'Pathway counts for iron: {pathway_counts}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"\"\"Monomer index: {C_names.index('SUPEROX-DISMUTFE-MONOMER')}, complex index: {C_names.index('SUPEROX-DISMUTFE-CPLX')}\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"\"\"Monomer count: {counts[5232]}, complex count: {counts[1021]}\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"\"\"Monomer index: {C_names.index('SUPEROX-DISMUTMN-MONOMER')}, complex index: {C_names.index('SUPEROX-DISMUTMN-CPLX')}\"\"\")\n",
    "print(f\"\"\"Monomer count: {counts[5233]}, complex count: {counts[1022]}\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mn_idx = element_ids.index('MN')\n",
    "mn_counts = (np.array(counts).reshape(-1, 1) * C @ P @ E)[:, mn_idx]\n",
    "# each complex count of 1 counts as 1 total annotation, so need to normalize\n",
    "C_n = C / (C.sum(axis=1, keepdims=1) + 1e-10)\n",
    "\n",
    "nz_idx = np.where((C_n @ W @ W2).T @ mn_counts)[0]\n",
    "mn_counts_group = (C_n @ W @ W2).T @ mn_counts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cplx_pathways = pathway_ids[ np.where((C @ W)[5233, :])[0][0] ]\n",
    "monomer_pathways = pathway_ids[ np.where((C @ W)[1022, :])[0][0] ]\n",
    "\n",
    "\n",
    "print(f'Superoxide dismutase is in the following pathways: {cplx_pathways}, while the monomer is in {monomer_pathways}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pathway_counts = (mn_counts @ C_n @ W @ W2)[  np.where((C @ W @ W2)[5233, :])[0][0]  ]\n",
    "pathway_counts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mn_counts[1022]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "counts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parsed_pathway_df[parsed_pathway_df['id'] == 'PWY0-1507']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pathway_ids.index('PWY0-1507')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.where(W2[753,:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pathway_ids[718]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## go annotation for proteins"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parsed_protein_df[parsed_protein_df['id'] == 'BCCP-MONOMER']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parsed_protein_df[parsed_protein_df['id'] == 'EG10889-MONOMER']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parsed_protein_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bulk.loc[:, 'RNA0-301[c]']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bulk.loc[:, 'NARH-MONOMER[j]']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ecocyc_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bulk.loc[:, 'NITRATREDUCTA-CPLX[i]']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for element in fba['estimated_fluxes']:\n",
    "    print(len(element))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fluxes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stoichiometry.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vars(metabolism).keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metabolism.reaction_names.index('RXN0-7077')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fluxes[:, 3470]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time = '2600'\n",
    "date = '2023-11-28'\n",
    "experiment = 'metabolism-redux-classic'\n",
    "entry = f'{experiment}_{time}_{date}'\n",
    "folder = f'out/cofactors/{entry}/'\n",
    "\n",
    "f = open(folder + 'agent_steps.pkl', 'rb')\n",
    "agent = dill.load(f)\n",
    "f.close()\n",
    "\n",
    "metabolism = agent['ecoli-metabolism-redux-classic']\n",
    "stoichiometry = metabolism.stoichiometry"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_all = np.load(folder + '0_output.npy',allow_pickle='TRUE').item()\n",
    "# output = np.load(r\"out/geneRxnVerifData/output_glc.npy\", allow_pickle=True, encoding='ASCII').tolist()\n",
    "output = output_all['agents']['0']\n",
    "fba = output['listeners']['fba_results']\n",
    "mass = output['listeners']['mass']\n",
    "bulk = pd.DataFrame(output['bulk'])\n",
    "\n",
    "fluxes = np.array(fba['estimated_fluxes'][1:])\n",
    "exchanges = fba['estimated_exchange_dmdt']\n",
    "\n",
    "output['listeners']['unique_molecule_counts']['active_ribosome']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flux_df = pd.DataFrame(fluxes, columns=metabolism.reaction_names)\n",
    "flux_df\n",
    "\n",
    "# find top 10 fluxes for timestep 50\n",
    "flux_df.loc[50, :].sort_values(ascending=False)[0:50]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exchanges.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exch_df_exp = pd.DataFrame([exchanges['GLC[p]'], exchanges['OXYGEN-MOLECULE[p]']]).T\n",
    "exch_df_exp.columns = ['GLC[p]', 'OXYGEN-MOLECULE[p]']\n",
    "exch_df_exp.to_csv('notebooks/cofactors/data/exchanges.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flux_df.loc[50, 'NADH-DEHYDROG-A-RXN-NADH/UBIQUINONE-8/PROTON//NAD/CPD-9956/PROTON.46.']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for name in metabolism.reaction_names:\n",
    "    if 'RXN0-5388' in name:\n",
    "        print(name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
